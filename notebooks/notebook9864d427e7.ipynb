{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T19:06:09.995514Z","iopub.execute_input":"2025-04-22T19:06:09.995820Z","iopub.status.idle":"2025-04-22T19:06:10.003954Z","shell.execute_reply.started":"2025-04-22T19:06:09.995796Z","shell.execute_reply":"2025-04-22T19:06:10.003288Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\n/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\n/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\n/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\n/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nLLM Detect AI Generated Text - Colab Baseline\n\nThis notebook provides a baseline solution for the Kaggle competition\n\"LLM - Detect AI Generated Text\", designed to run on Google Colab's free tier.\nIt fine-tunes a DeBERTa-v3-base model.\n\"\"\"\n\n# -----------------------------------------------------------------------------\n# 1. Setup & Installs\n# -----------------------------------------------------------------------------\n!pip install -q transformers datasets evaluate accelerate scikit-learn pandas\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport gc # Garbage collector\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    EarlyStoppingCallback\n)\nfrom datasets import Dataset, DatasetDict\n\nprint(\"Libraries imported.\")\n\n# Check GPU availability and clear cache (good practice on Colab)\nif torch.cuda.is_available():\n    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n    torch.cuda.empty_cache()\n    USE_FP16 = True # Enable Mixed Precision if T4/P100/V100 is available\n    # Note: K80 GPUs on Colab might not support FP16 well.\n    # The Trainer will often handle this, but manual checks can be added if needed.\nelse:\n    print(\"No GPU detected, running on CPU (will be very slow). FP16 disabled.\")\n    USE_FP16 = False\n\n# -----------------------------------------------------------------------------\n# 2. Configuration\n# -----------------------------------------------------------------------------\nclass CFG:\n    MODEL_NAME = \"microsoft/deberta-v3-base\" # Good balance of performance/size\n    # MODEL_NAME = \"roberta-base\" # Alternative good choice\n    MAX_LENGTH = 512        # Max sequence length for tokenizer\n    TRAIN_BATCH_SIZE = 8    # Adjust based on GPU VRAM (4, 8, 16 are common)\n    EVAL_BATCH_SIZE = 16\n    GRAD_ACCUM_STEPS = 2    # Effective batch size = TRAIN_BATCH_SIZE * GRAD_ACCUM_STEPS\n    LEARNING_RATE = 2e-5\n    EPOCHS = 3              # Start with a few epochs, increase if needed/possible\n    WEIGHT_DECAY = 0.01\n    RANDOM_SEED = 42\n    VAL_SPLIT = 0.2         # Use 20% of training data for validation\n    OUTPUT_DIR = \"llm-detect-output\"\n    DATA_PATH = \"/kaggle/input/llm-detect-ai-generated-text/\" # Default Kaggle path\n\n# Set seed for reproducibility\nnp.random.seed(CFG.RANDOM_SEED)\ntorch.manual_seed(CFG.RANDOM_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(CFG.RANDOM_SEED)\n\n# -----------------------------------------------------------------------------\n# 3. Load & Prepare Data\n# -----------------------------------------------------------------------------\nprint(\"Loading data...\")\n# --- IMPORTANT: Adapt this path if running locally or on Colab ---\n# If on Colab, you might need to upload data or mount Google Drive\n# Example for Colab if data is uploaded to session storage:\n# CFG.DATA_PATH = \"./\" # Assuming files are in the root runtime directory\n\n# Load the primary training data\ntry:\n    # Assuming execution within a Kaggle Notebook environment\n    train_essays = pd.read_csv(f\"{CFG.DATA_PATH}train_essays.csv\")\n    test_essays = pd.read_csv(f\"{CFG.DATA_PATH}test_essays.csv\")\n    sample_submission = pd.read_csv(f\"{CFG.DATA_PATH}sample_submission.csv\")\n\n    # Load external dataset (if available and beneficial)\n    # Be mindful of Colab disk space!\n    try:\n         # This specific dataset was commonly used in this competition\n         external_df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n         # Prepare external data similar to train_essays if structure differs\n         # Example: Rename columns if necessary, select relevant text\n         # For simplicity here, assume it has 'text' and 'label' columns\n         if 'text' not in external_df.columns or 'label' not in external_df.columns:\n             print(\"Warning: External dataset columns might need adjustment.\")\n             # Add specific renaming/selection logic here if needed\n             external_df = external_df[['text', 'label']].copy() # Basic example\n\n         print(f\"Loaded external dataset with {len(external_df)} rows.\")\n         # Combine with train_essays\n         train_essays = pd.concat([\n    # 1. Select 'text', 'generated' from train_essays and rename 'generated' to 'label'\n    train_essays[['text', 'generated']].rename(columns={'generated': 'label'}),\n\n    # 2. Use all columns from external_df as they are\n    external_df\n\n], ignore_index=True)\n         # This will display all rows where the 'label' column is NaN\n         print(\"Rows with NaN in 'label' column:\")\n         print(train_essays[train_essays['label'].isnull()])\n         # Make sure label column is consistent (0 or 1)\n         train_essays['label'] = train_essays['label'].astype(int)\n         print(f\"Combined training data shape: {train_essays.shape}\")\n\n    except FileNotFoundError:\n        print(\"Optional external dataset not found. Proceeding with train_essays only.\")\n        # Rename 'generated' column to 'label' for consistency\n        train_essays.rename(columns={'generated': 'label'}, inplace=True)\n\nexcept FileNotFoundError:\n    print(\"ERROR: Could not find competition data.\")\n    print(f\"Looked in: {CFG.DATA_PATH}\")\n    print(\"Please ensure the data files (train_essays.csv, test_essays.csv) are accessible.\")\n    # You might need to upload manually or adjust the DATA_PATH if not on Kaggle\n    # Example for local execution: CFG.DATA_PATH = \"./data/\"\n    raise SystemExit(\"Data files not found.\")\n\n\nprint(f\"Train essays shape: {train_essays.shape}\")\nprint(f\"Test essays shape: {test_essays.shape}\")\nprint(train_essays.head())\n\n# Split training data for validation\nprint(f\"Splitting data (Validation size: {CFG.VAL_SPLIT})...\")\ntrain_df, val_df = train_test_split(\n    train_essays,\n    test_size=CFG.VAL_SPLIT,\n    random_state=CFG.RANDOM_SEED,\n    stratify=train_essays['label'] # Important for classification\n)\n\nprint(f\"Train split shape: {train_df.shape}\")\nprint(f\"Validation split shape: {val_df.shape}\")\n\n# Convert pandas DataFrames to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_essays) # Keep test separate\n\n# Create a DatasetDict if needed (optional, but good practice)\n# ds = DatasetDict({\n#     'train': train_dataset,\n#     'validation': val_dataset,\n#     'test': test_dataset\n# })\n# print(ds)\n\n# Clean up memory\ndel train_df, val_df, train_essays\ngc.collect()\n\n# -----------------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n# 4. Tokenization\n# -----------------------------------------------------------------------------\nprint(f\"Loading tokenizer: {CFG.MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n\ndef tokenize_function(examples):\n    \"\"\"Applies tokenizer to text data.\"\"\"\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=False, # Pad dynamically later with DataCollator\n        max_length=CFG.MAX_LENGTH,\n        return_token_type_ids=False, # Not needed for DeBERTa v3/RoBERTa\n    )\n\nprint(\"Tokenizing datasets...\")\n\n# --- Dynamically determine columns to remove ---\n# It's safer to check actual columns before removing\n\n# For train/val: remove text (used by tokenizer) and any extra columns\n# that came from concatenation (like __index_level_0__ or from external_df)\n# DO NOT try to remove 'id' as it's not present after the concat operation.\ntrain_val_potential_remove = ['text', '__index_level_0__']\n# Add columns from external_df if they exist and aren't needed (check external_df source)\n# Example: train_val_potential_remove.extend(['prompt_name', 'source', 'RDizzl3_seven'])\n\ntrain_cols_to_remove = [col for col in train_val_potential_remove if col in train_dataset.column_names]\nval_cols_to_remove = [col for col in train_val_potential_remove if col in val_dataset.column_names]\n\n# For test: remove only 'text', keep 'id' for submission\ntest_cols_to_remove = ['text']\nif 'text' not in test_dataset.column_names:\n     print(\"Warning: 'text' column not found in test_dataset before mapping.\")\n     test_cols_to_remove = [] # Avoid error if text somehow missing\n\nprint(f\"Columns to remove from train_dataset: {train_cols_to_remove}\")\nprint(f\"Columns to remove from val_dataset: {val_cols_to_remove}\")\nprint(f\"Columns to remove from test_dataset: {test_cols_to_remove}\")\n\n\n# Apply tokenization with the CORRECTED lists\ntrain_tokenized = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=train_cols_to_remove # Use calculated list\n)\nval_tokenized = val_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=val_cols_to_remove   # Use calculated list\n)\ntest_tokenized = test_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=test_cols_to_remove    # Only removes 'text'\n)\n\n# Data collator handles dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nprint(\"Tokenization complete.\")\n# print(train_tokenized[0]) # Check an example\n# print(test_tokenized[0]) # Check an example, 'id' should be present if it was in test_essays\n\n# -----------------------------------------------------------------------------\n# 5. Model Loading & Training Setup\n# -----------------------------------------------------------------------------\nprint(f\"Loading model: {CFG.MODEL_NAME}\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    CFG.MODEL_NAME,\n    num_labels=2, # Binary classification (human=0, AI=1)\n    # ignore_mismatched_sizes=True # Use if encountering checkpoint size issues (less common now)\n)\n\n# Define metrics - AUC is the competition metric\ndef compute_metrics(eval_pred):\n    \"\"\"Computes AUC score for evaluation.\"\"\"\n    predictions, labels = eval_pred\n    # Sigmoid / Softmax needed depending on model output type\n    # Trainer often handles logits, so apply softmax or sigmoid\n    # If logits:\n    # probs = torch.nn.functional.softmax(torch.Tensor(predictions), dim=-1)[:, 1].numpy()\n    # If probabilities directly (less common from Trainer):\n    # probs = predictions[:, 1]\n\n    # Assuming predictions are logits (most common from HF Trainer)\n    if isinstance(predictions, tuple): # Handle potential tuple output\n        logits = predictions[0]\n    else:\n        logits = predictions\n    \n    probs = torch.tensor(logits).softmax(dim=-1)[:, 1].numpy() # Probability of class 1\n    \n    auc = roc_auc_score(labels, probs)\n    # You could add accuracy, f1 etc. here too\n    # from sklearn.metrics import accuracy_score\n    # preds = np.argmax(logits, axis=1)\n    # acc = accuracy_score(labels, preds)\n    return {\n        'auc': auc,\n        # 'accuracy': acc\n    }\n\nprint(\"Setting up Training Arguments...\")\ntraining_args = TrainingArguments(\n    output_dir=CFG.OUTPUT_DIR,\n    learning_rate=CFG.LEARNING_RATE,\n    per_device_train_batch_size=CFG.TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=CFG.EVAL_BATCH_SIZE,\n    gradient_accumulation_steps=CFG.GRAD_ACCUM_STEPS,\n    num_train_epochs=CFG.EPOCHS,\n    weight_decay=CFG.WEIGHT_DECAY,\n    eval_strategy=\"epoch\",        # Evaluate every epoch\n    save_strategy=\"epoch\",              # Save model every epoch\n    load_best_model_at_end=True,        # Load the best model based on metric\n    metric_for_best_model=\"auc\",        # Competition metric\n    greater_is_better=True,\n    fp16=USE_FP16,                      # Enable Mixed Precision\n    # fp16_full_eval=USE_FP16,          # Use FP16 for evaluation too (if memory allows)\n    logging_strategy=\"steps\",           # Log periodically\n    logging_steps=50,                  # Log every 50 steps\n    report_to=\"none\",                   # Disable WandB/Tensorboard logging for simplicity\n    save_total_limit=1,                 # Only keep the best checkpoint\n    seed=CFG.RANDOM_SEED,\n    # dataloader_num_workers=2,         # Can sometimes speed up data loading\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tokenized,\n    eval_dataset=val_tokenized,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if no improvement for 3 epochs\n)\n\n# Clean cache before training\ngc.collect()\ntorch.cuda.empty_cache()\n\n# -----------------------------------------------------------------------------\n# 6. Train the Model\n# -----------------------------------------------------------------------------\nprint(\"Starting training...\")\ntrain_result = trainer.train()\n\n# Print training summary\nmetrics = train_result.metrics\ntrainer.log_metrics(\"train\", metrics)\ntrainer.save_metrics(\"train\", metrics)\nprint(\"\\nTraining complete.\")\n\n# -----------------------------------------------------------------------------\n# 7. Evaluate on Validation Set\n# -----------------------------------------------------------------------------\nprint(\"\\nEvaluating on validation set...\")\neval_metrics = trainer.evaluate()\ntrainer.log_metrics(\"eval\", eval_metrics)\ntrainer.save_metrics(\"eval\", eval_metrics)\nprint(f\"Validation Metrics: {eval_metrics}\")\n\n# Optional: Save the best model explicitly (Trainer already saves it)\n# trainer.save_model(f\"{CFG.OUTPUT_DIR}/best_model\")\n# tokenizer.save_pretrained(f\"{CFG.OUTPUT_DIR}/best_model\")\n\n# -----------------------------------------------------------------------------\n# 8. Generate Predictions for Submission\n# -----------------------------------------------------------------------------\nprint(\"\\nGenerating predictions on the test set...\")\n\n# Ensure model is on the correct device (GPU if available)\nif torch.cuda.is_available():\n    model.to('cuda')\n\n# Get predictions\n# The trainer.predict() method returns prediction outputs, which include logits\npredictions_output = trainer.predict(test_tokenized)\ntest_logits = predictions_output.predictions\n\n# Convert logits to probabilities (probability of being AI-generated, i.e., class 1)\ntest_probs = torch.tensor(test_logits).softmax(dim=-1)[:, 1].numpy()\n\n# Create submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_essays['id'],\n    'generated': test_probs\n})\n\n# Save submission file\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\nSubmission file created: submission.csv\")\nprint(submission_df.head())\n\n# -----------------------------------------------------------------------------\n# 9. Clean up\n# -----------------------------------------------------------------------------\nprint(\"\\nCleaning up memory...\")\ndel model, tokenizer, trainer, train_tokenized, val_tokenized, test_tokenized\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"Done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T19:06:10.410242Z","iopub.execute_input":"2025-04-22T19:06:10.410494Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Libraries imported.\nGPU detected: Tesla T4\nLoading data...\nLoaded external dataset with 44868 rows.\nRows with NaN in 'label' column:\nEmpty DataFrame\nColumns: [text, label, prompt_name, source, RDizzl3_seven]\nIndex: []\nCombined training data shape: (46246, 5)\nTrain essays shape: (46246, 5)\nTest essays shape: (3, 3)\n                                                text  label prompt_name  \\\n0  Cars. Cars have been around since they became ...      0         NaN   \n1  Transportation is a large necessity in most co...      0         NaN   \n2  \"America's love affair with it's vehicles seem...      0         NaN   \n3  How often do you ride in a car? Do you drive a...      0         NaN   \n4  Cars are a wonderful thing. They are perhaps o...      0         NaN   \n\n  source RDizzl3_seven  \n0    NaN           NaN  \n1    NaN           NaN  \n2    NaN           NaN  \n3    NaN           NaN  \n4    NaN           NaN  \nSplitting data (Validation size: 0.2)...\nTrain split shape: (36996, 5)\nValidation split shape: (9250, 5)\nLoading tokenizer: microsoft/deberta-v3-base\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Tokenizing datasets...\nColumns to remove from train_dataset: ['text', '__index_level_0__']\nColumns to remove from val_dataset: ['text', '__index_level_0__']\nColumns to remove from test_dataset: ['text']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36996 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d57d178e4a04d1aa9decf0ec6c4c4fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73d905e78164487926b456a29296459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6600571af77346dfb6ff7df3f862559d"}},"metadata":{}},{"name":"stdout","text":"Tokenization complete.\nLoading model: microsoft/deberta-v3-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_31/1991845737.py:292: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Setting up Training Arguments...\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='101' max='3468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 101/3468 03:54 < 2:13:05, 0.42 it/s, Epoch 0.09/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null}]}